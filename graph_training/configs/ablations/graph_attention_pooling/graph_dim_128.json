{
  "experiment_name": "graph_dim_128",
  "mlp" : {
    "fc_layers_num": 2,
    "num_graphs" : 10,
    "use_pool" : true,
    "use_proj" : true,
    "action_graph_embedder" : {
      "emb_dim" : 32,
      "obj_out" : 32,
      "aux_out" : 16,
      "attr_out" : 32,
      "rel_out" : 32,
      "trip_out" :  32,
      "k_obj" :  2,
      "k_aux" : 2,
      "k_trip" :  2,
      "clip_dim" : 1280,
      "clip_emb_dim" : 640,
      "obj_feat_dim" : 256  

    },
    "projector" : {
      "graph_emb_dim" : 256,
      "layer_norm" : true,
      "gelu" : true
    },
    "attention_pooler" : {
      "graph_pool_interim_feat" : 256,
      "final_graph_emb_dim" : 128
    }
  },
  "data": {
    "input_path" : "/home/s3758869/vlm_datasets/AriaEA_vlm_ann_3_10_llava-v1.6-34b-hf",
    "batch_size": 1,
    "num_workers": 16,
    "pin_memory": true
  },
  "training": {
    "num_epochs": 20,
    "optimizer": "adam",
    "learning_rate": 0.0005,
    "weight_decay": 1e-05,
    "scheduler_factor": 1.0,
    "criterion_metrics": ["f1", "acc"],
    "loss": {
      "name": "cross_entropy",
      "ifw": false,
      "focal_gamma": 2.0
    }
  },
  "output": {
    "base_path": "/home/s3758869/egocentric_video_graph_framework_ar/outputs/graph_mlp_baseline/ablation/graph_attention_pooling"
  },
  "device": "cuda"
}
