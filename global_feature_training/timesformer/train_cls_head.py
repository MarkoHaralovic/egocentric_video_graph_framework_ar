import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'TimeSformer'))
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../..'))
import torch
import json
from tqdm import tqdm
from torch.utils.data import DataLoader
 
from global_feature_training.base_model.sequence_model import ImageSequenceClassificator
from global_feature_training.timesformer.timesformer_feature_extraction import (
    get_timesformer_visual_backbone,
    get_clip_text_encoder
)
from global_feature_training.data_loading.sequence_dataset import SequenceDataset, feature_collate_fn
from global_feature_training.data_loading.dataset_split import return_train_val_samples
from global_feature_training.train.train import do_epoch
from global_feature_training.train.evaluate import store_model

config_path = "/home/s3758869/egocentric_video_graph_framework_ar/global_feature_training/timesformer/configs/run_config.json"
with open(config_path, 'r') as f:
    config = json.load(f)

experiment_name = config["experiment_name"]
print(f"Running experiment: {experiment_name}")

USE_PRECOMPUTED_FEATURES = config["model"]["use_precomputed_features"]
TIMESFORMER_PRECOMUPTED_FEATS_FOLDER = config["data"]["precomputed_features_folder"]
# data_split_path = config["data"]["split_json_path"]
timesformer_model_name = config["model"]["name"]
timesformer_model_path = config["model"]["timesformer_model_path"]
clip_model_path = config["model"]["clip_model_path"]
clip_cache_dir = config["model"]["clip_cache_dir"]
device = config["device"]

num_epochs = config["training"]["num_epochs"]
fc_layers_num = config["training"]["fc_layers_num"]
optimizer_name = config["training"]["optimizer"]
learning_rate = config["training"]["learning_rate"]
weight_decay = config["training"]["weight_decay"]
scheduler_factor = config["training"]["scheduler_factor"]
criterion_metrics = config["training"]["criterion_metrics"]
batch_size = config["data"]["batch_size"]
num_workers = config["data"]["num_workers"]
use_text_features = config["model"]["use_text_features"]

# with open(data_split_path,'r') as f:
#    data_split = json.load(f)

pooling =None
model_name = "timesformer_k600_8fr_224"
num_frames = 8

train_samples, val_samples, activity_to_idx = return_train_val_samples(
   model_name=model_name, 
   num_frames = num_frames,
   pooling=pooling, 
   noun_replacement="other",
   skip_na=True,
)

print(f"activity_to_idx : {activity_to_idx}")
print(f"len(train_samples) : {len(train_samples)}")
print(f"len(val_samples) : {len(val_samples)}")


train_dataset = SequenceDataset(
    input_folder = TIMESFORMER_PRECOMUPTED_FEATS_FOLDER,
    clip_names = None, #data_split["split_summary"]["train"],
    samples= train_samples,
    model_name = timesformer_model_name,
    pooling = pooling,
    frames = num_frames,
    load_visual=True,
    load_text=False,
    activity_to_idx = activity_to_idx
)

validation_dataset = SequenceDataset(
    input_folder = TIMESFORMER_PRECOMUPTED_FEATS_FOLDER,
    clip_names = None, #data_split["split_summary"]["validation"],
    samples = val_samples,
    model_name = timesformer_model_name,
    pooling = pooling,
    frames=num_frames,
    load_visual=True,
    load_text=False,
    activity_to_idx = activity_to_idx
)

assert train_dataset.activity_to_idx == validation_dataset.activity_to_idx
cls_mapping = train_dataset.activity_to_idx

train_loader = DataLoader(
    train_dataset,
    batch_size=batch_size,
    shuffle=True,
    num_workers=num_workers,
    pin_memory=True,
    collate_fn=feature_collate_fn
)

val_loader = DataLoader(
    validation_dataset,
    batch_size=batch_size,
    shuffle=False,
    num_workers=num_workers,
    pin_memory=True,
    collate_fn=feature_collate_fn
)


if not USE_PRECOMPUTED_FEATURES:
    vision_backbone = get_timesformer_visual_backbone(
        img_size=224,
        num_classes=600,
        num_frames=num_frames,
        attention_type='divided_space_time',
        pretrained_model=timesformer_model_path
    )
    tokenizer, text_backbone, _ = get_clip_text_encoder(clip_model_path, clip_cache_dir)
else:
    vision_backbone = None
    text_backbone = None


linear_layer_input_dim = config["model"]["linear_layer_input_dim"]

model = ImageSequenceClassificator(
    vision_backbone=vision_backbone,
    text_backbone=text_backbone,
    n_classes=len(cls_mapping),
    linear_layer_input_dim=linear_layer_input_dim,
    fc_layers_num=fc_layers_num,
    use_precomputed_features=USE_PRECOMPUTED_FEATURES,
    transforms=None,
    device=device
).to(device)

if optimizer_name == 'adam':
    opt = torch.optim.Adam(
        params=model.parameters(),
        lr=learning_rate,
        weight_decay=weight_decay,
    )
elif optimizer_name == 'sgd':
    opt = torch.optim.SGD(
        params=model.parameters(),
        lr=learning_rate,
        momentum=0.9,
        weight_decay=weight_decay,
    )

scheduler = None
if scheduler_factor != 1.0:
    scheduler = torch.optim.lr_scheduler.MultiplicativeLR(
        opt, lr_lambda=lambda epoch: scheduler_factor
    )

save_path = os.path.join(
    config["output"]["base_path"],
    f"timesformer_fc_layer_{fc_layers_num}_num_epoch_{num_epochs}"
)
os.makedirs(save_path, exist_ok=True)

results = {}
best_epoch_result = {"acc": -1, "f1": -1}
global_step = 0

experiment_config = {
    "experiment_name": experiment_name,
    "model": timesformer_model_name,
    "fc_layers_num": fc_layers_num,
    "num_epochs": num_epochs,
    "batch_size": batch_size,
    "learning_rate": learning_rate,
    "weight_decay": weight_decay,
    "optimizer": optimizer_name,
    "scheduler_factor": scheduler_factor,
    "num_classes": len(cls_mapping),
    "use_precomputed_features": USE_PRECOMPUTED_FEATURES,
    "use_text_features": use_text_features,
    "device": device,
    "linear_layer_input_dim": linear_layer_input_dim
}

with open(os.path.join(save_path, "experiment_config.json"), "w") as f:
    json.dump(experiment_config, f, indent=2)

with open(os.path.join(save_path, "class_mapping.json"), "w") as f:
    json.dump(cls_mapping, f, indent=2)
            
for epoch in tqdm(range(num_epochs), desc=f"Training for {num_epochs} epochs", unit="epoch", total=num_epochs):
    print(f"Epoch {epoch+1}/{num_epochs}\n")
    
    epoch_result, global_step = do_epoch(
        device=device,
        net=model,
        opt=opt,
        train_loader=train_loader,
        validate_loader=val_loader,
        global_step=global_step,
        num_classes_train=len(cls_mapping),
        num_classes_val=len(validation_dataset.activity_to_idx),
        text_features=use_text_features
    )
    
    val_metrics = epoch_result["val"]["eval_metrics"]
    train_metrics = epoch_result["train"]["eval_metrics"]
    
    print(f"\nValidation - Accuracy: {val_metrics['acc']*100:.2f}%, F1: {val_metrics['f1']*100:.2f}%")
    
    if val_metrics["acc"] > best_epoch_result["acc"]:
        best_epoch_result["acc"] = val_metrics["acc"]
        store_model(
            net=model,
            opt=opt,
            epoch=epoch,
            save_path=save_path,
            metric="acc"
        )
        print(f"New best accuracy model saved: {val_metrics['acc']*100:.2f}%")
        
    if val_metrics["f1"] > best_epoch_result["f1"]:
        best_epoch_result["f1"] = val_metrics["f1"]
        store_model(
            net=model,
            opt=opt,
            epoch=epoch,
            save_path=save_path,
            metric="f1"
        )
        print(f"New best F1 model saved: {val_metrics['f1']*100:.2f}%")
    
    results[epoch] = epoch_result
    
    if scheduler is not None:
        scheduler.step()
    
    print(f"Epoch {epoch+1} completed")

with open(os.path.join(save_path, "training_results.json"), "w") as f:
    json_results = {}
    for ep, res in results.items():
        json_results[str(ep)] = {
            "train": {"acc": float(res["train"]["eval_metrics"]["acc"]), 
                     "f1": float(res["train"]["eval_metrics"]["f1"])},
            "val": {"acc": float(res["val"]["eval_metrics"]["acc"]), 
                   "f1": float(res["val"]["eval_metrics"]["f1"])}
        }
    json.dump(json_results, f, indent=2)

print(f"Best validation accuracy: {best_epoch_result['acc']*100:.2f}%")
print(f"Best validation F1: {best_epoch_result['f1']*100:.2f}%")